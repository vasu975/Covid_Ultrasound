{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "Copyright (c) 2021, Her Majesty in Right of Canada as represented by the National Research Council Canada. Rights provided under GNU GENERAL PUBLIC LICENSE, Version 3. Full text of the license accessible at the [LICENSE](LICENSE) file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scripts automatically collect videos from the following sources, processes and curates them, and stores them along with their metadata on the user's device:\n",
    "\n",
    "* ButterflyNetwork\n",
    "* GrepMed\n",
    "* LITFL\n",
    "* The PocusAtlas\n",
    "* Radiopaedia\n",
    "* CoreUltrasound\n",
    "* University of Florida (UF)\n",
    "* Scientific Publications\n",
    "* Clarius\n",
    "\n",
    "In addition, it extracts images from the collected videos, processes and curates them, and stores images and their metadata locally on the user's device.\n",
    "\n",
    "__Note:__ No data is stored on the NRC-COVIDx-US repository and it only contains scripts to systematically collect, curate, and integrate data on user's device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import random \n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "from vimeo_downloader import Vimeo\n",
    "import urllib.request\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "import time\n",
    "from image_data import extract_images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import subprocess # to unzip butterfly file\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 1.4.1\n",
      "selenium 4.1.0\n",
      "requests 2.27.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Pandas\", pd.__version__)\n",
    "import selenium\n",
    "print(\"selenium\", selenium.__version__)\n",
    "print(\"requests\", requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_download_path():\n",
    "    \"\"\"Returns the default downloads path for linux or windows\"\"\"\n",
    "    if os.name == 'nt':\n",
    "        import winreg\n",
    "        sub_key = r'SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders'\n",
    "        downloads_guid = '{374DE290-123F-4565-9164-39C4925E467B}'\n",
    "        with winreg.OpenKey(winreg.HKEY_CURRENT_USER, sub_key) as key:\n",
    "            location = winreg.QueryValueEx(key, downloads_guid)[0]\n",
    "        return location\n",
    "    else:\n",
    "        return os.path.join(os.path.expanduser('~'), 'downloads')\n",
    "    \n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Function to remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to download zip files from Google drive, in case required\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    progress = ProgressBar() \n",
    "    \n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in progress(response.iter_content(CHUNK_SIZE)):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set save path directory\n",
    "SAVE_PATH = 'data'\n",
    "\n",
    "# create data, video, and image folders, if they do not exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "if not os.path.exists('data/video'):\n",
    "    os.makedirs('data/video')\n",
    "if not os.path.exists('data/image'):\n",
    "    os.makedirs('data/image')\n",
    "    \n",
    "# setting chrome driver\n",
    "chromedriver = \"utils/chromedriver.exe\" \n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# setting global vars\n",
    "VIDEO_PATH = 'data/video/'\n",
    "IMAGE_PATH = 'data/image/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filetype</th>\n",
       "      <th>folder</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>probe</th>\n",
       "      <th>class</th>\n",
       "      <th>class_on_website</th>\n",
       "      <th>version</th>\n",
       "      <th>...</th>\n",
       "      <th>type</th>\n",
       "      <th>patient</th>\n",
       "      <th>case_no</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>comment</th>\n",
       "      <th>paper_link</th>\n",
       "      <th>paper_doi</th>\n",
       "      <th>license</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_butterfly_covid</td>\n",
       "      <td>Coalescing B lines.mp4</td>\n",
       "      <td>mp4</td>\n",
       "      <td>data\\tmp\\Butterfly\\B lines</td>\n",
       "      <td>Butterfly</td>\n",
       "      <td>https://butterflynetwork.getbynder.com/transfe...</td>\n",
       "      <td>Convex</td>\n",
       "      <td>COVID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>lung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_butterfly_covid</td>\n",
       "      <td>Confluent B lines.mp4</td>\n",
       "      <td>mp4</td>\n",
       "      <td>data\\tmp\\Butterfly\\B lines</td>\n",
       "      <td>Butterfly</td>\n",
       "      <td>https://butterflynetwork.getbynder.com/transfe...</td>\n",
       "      <td>Convex</td>\n",
       "      <td>COVID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>lung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                filename filetype  \\\n",
       "0  1_butterfly_covid  Coalescing B lines.mp4      mp4   \n",
       "1  2_butterfly_covid   Confluent B lines.mp4      mp4   \n",
       "\n",
       "                       folder     source  \\\n",
       "0  data\\tmp\\Butterfly\\B lines  Butterfly   \n",
       "1  data\\tmp\\Butterfly\\B lines  Butterfly   \n",
       "\n",
       "                                                 url   probe  class  \\\n",
       "0  https://butterflynetwork.getbynder.com/transfe...  Convex  COVID   \n",
       "1  https://butterflynetwork.getbynder.com/transfe...  Convex  COVID   \n",
       "\n",
       "  class_on_website  version  ...  type patient case_no  gender age  comment  \\\n",
       "0              NaN      1.0  ...  lung     NaN     NaN     NaN NaN      NaN   \n",
       "1              NaN      1.0  ...  lung     NaN     NaN     NaN NaN      NaN   \n",
       "\n",
       "  paper_link paper_doi license link  \n",
       "0        NaN       NaN     NaN  NaN  \n",
       "1        NaN       NaN     NaN  NaN  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('utils/video_metadata.csv', sep=',', encoding='latin1')\n",
    "print(metadata.shape)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Ultrasound Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ButterflyNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note1:__ Depending on your system configuration the download button may not load in time. If you get an error, increase the sleep time in the following code:\n",
    "\n",
    "```python\n",
    "time.sleep(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note2:__ The below code block works with Chrome web browser and with ChromeDriver version 88 that is included in the utils folder. Depending on the version of your chrome browser, you may get a ChromeDriver version error. If occurs, please download the correct version of ChromeDriver based on the version of your chrome browser from this [link](https://chromedriver.chromium.org/downloads), and copy it to the utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Downloading ButterflyNetwork zip file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c17463f21b25>:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(chromedriver) #, options=chrome_options)\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_line_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             self.process = subprocess.Popen(cmd, env=self.env,\n\u001b[0m\u001b[0;32m     72\u001b[0m                                             \u001b[0mclose_fds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c17463f21b25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# simulatting button click to download the zip file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mbrowser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchromedriver\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, options=chrome_options)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbutterfly_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mservice_log_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         super(WebDriver, self).__init__(DesiredCapabilities.CHROME['browserName'], \"goog\",\n\u001b[0m\u001b[0;32m     71\u001b[0m                                         \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                                         \u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_capabilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 raise WebDriverException(\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m     83\u001b[0m                         os.path.basename(self.path), self.start_error_message)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n"
     ]
    }
   ],
   "source": [
    "# zip file url\n",
    "butterfly_url = metadata[(metadata.source == 'Butterfly') & (metadata.date_added == 'Mar_2021')].url.unique()[0]\n",
    "print('...Downloading ButterflyNetwork zip file...')\n",
    "\n",
    "# simulatting button click to download the zip file\n",
    "browser = webdriver.Chrome(chromedriver) #, options=chrome_options)\n",
    "browser.get(butterfly_url)\n",
    "\n",
    "# Download button sometimes doesn't load in time to click. If such error occurring, increase sleep time\n",
    "time.sleep(5)\n",
    "\n",
    "browser.find_element_by_class_name('btn-primary').click() \n",
    "\n",
    "# path to the downloaded zip file\n",
    "zip_file_path = os.path.join(get_download_path(), 'Published -20210112T164653Z-001.zip')  # new version - checked March 9, 2021\n",
    "\n",
    "# wait till the zip file is downloaded\n",
    "while not os.path.exists(zip_file_path):\n",
    "    time.sleep(1)\n",
    "\n",
    "# create butterfly folder under video folder, if it does not exist\n",
    "if not os.path.exists('data/tmp/Butterfly'):\n",
    "    os.makedirs('data/tmp/Butterfly')\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "print('...Extracting the video files...')\n",
    "# extract the downloaded zip file and remove the zip file after extraction\n",
    "os.rename(zip_file_path, zip_file_path.replace(' ', ''))\n",
    "subprocess.Popen(\"utils/7z.exe\" +' x ' + zip_file_path.replace(' ', '') + ' -o' + 'data/tmp/Butterfly',stdout=subprocess.PIPE)\n",
    "time.sleep(5)\n",
    "\n",
    "# copy files from subfolders to the video folder\n",
    "for root, dirs, files in os.walk('data/tmp/Butterfly/Published_'):  \n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            continue\n",
    "        path_file = os.path.join(root,file)\n",
    "        shutil.copy2(path_file, 'data/video') \n",
    "\n",
    "# renaming extracted files to their ids\n",
    "progress = ProgressBar() \n",
    "for root, dirs, files in os.walk('data/video'):  \n",
    "    for file in progress(files):\n",
    "        if file.endswith(\".png\"):\n",
    "            continue\n",
    "        path_file = os.path.join(root,file)\n",
    "        file_id = metadata[metadata.filename == file].id.values[0] + '.mp4'\n",
    "        # rename the file to its id\n",
    "        os.rename(path_file, os.path.join(root,file_id))\n",
    "\n",
    "print('=== ButterflyNetwork video files extraction done! ===')        \n",
    "        \n",
    "# delete the tmp folder and its contents\n",
    "shutil.rmtree('data/tmp')\n",
    "        \n",
    "# remove the zip file\n",
    "os.remove(zip_file_path.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you get error running the above cell, uncomment and run the following code block instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# print('Downloading Butterfly zip file')\n",
    "# file_id = '18I4N6lWdcUW618Qwr6Krsd1Rkn946Ag0' # sharable link id\n",
    "# zip_file_path = os.path.join(get_download_path(), 'butterfly.zip')\n",
    "# download_file_from_google_drive(file_id, zip_file_path)\n",
    "\n",
    "# # unzip video files\n",
    "# print('Download complete\\nExtracting video files')\n",
    "# open_file = subprocess.Popen(\"utils/7z.exe\" +' x ' + zip_file_path + ' -o' + 'data/video',stdout=subprocess.PIPE)\n",
    "# print('Extraction complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. GrepMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "grepmed_df = metadata[metadata.source == 'GrepMed']\n",
    "\n",
    "progress = ProgressBar(max_value=grepmed_df.shape[0]) \n",
    "for idx, row in progress(grepmed_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    # write the video file to disk\n",
    "    vid = requests.get(row.url).content\n",
    "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n",
    "        handler.write(vid)\n",
    "print('=== GrepMed video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. LITFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "litfl_df = metadata[metadata.source == 'Litfl']\n",
    "\n",
    "progress = ProgressBar(max_value=litfl_df.shape[0]) \n",
    "for idx, row in progress(litfl_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    # write the video file to disk\n",
    "    vid = requests.get(row.url).content\n",
    "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n",
    "        handler.write(vid)\n",
    "print('=== LITFL video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. The POCUS Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "pocus_df = metadata[metadata.source == 'PocusAtlas']\n",
    "\n",
    "progress = ProgressBar(max_value=pocus_df.shape[0]) \n",
    "for idx, row in progress(pocus_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    # write the video file to disk\n",
    "    vid = requests.get(row.url).content\n",
    "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n",
    "        handler.write(vid)\n",
    "print('=== THEPocusAtlas video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Radiopaedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "radio_df = metadata[metadata.source == 'Radiopaedia']\n",
    "\n",
    "progress = ProgressBar(max_value=radio_df.shape[0]) \n",
    "for idx, row in progress(radio_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    # write the video file to disk\n",
    "    vid = requests.get(row.url).content\n",
    "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n",
    "        handler.write(vid)\n",
    "print('=== Radiopaedia video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. CoreUltrasound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "core_df = metadata[metadata.source == 'CoreUltrasound']\n",
    "\n",
    "progress = ProgressBar(max_value=core_df.shape[0]) \n",
    "for idx, row in progress(core_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    \n",
    "    # extract videos from Vimeo\n",
    "    if 'vimeo' in row.url:\n",
    "        v = Vimeo(row.url)\n",
    "        stream = v.streams # List of available streams of different quality\n",
    "        highest_quality_available = stream[-1]\n",
    "        highest_quality_available.download(download_directory = 'data/video/', filename = filename.split('.')[0])\n",
    "    # extract mp4 videos\n",
    "    else:\n",
    "        # write the video file to disk\n",
    "        vid = requests.get(row.url).content\n",
    "        with open(os.path.join('data/video/', filename), 'wb') as handler:\n",
    "            handler.write(vid)\n",
    "print('=== CoreUltrasound video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. UF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = metadata[(metadata.source == 'Paper') & ((metadata['id'].str.contains('199', na=False)) | (metadata['id'].str.contains('200', na=False)))] \n",
    "\n",
    "progress = ProgressBar(max_value=paper_df.shape[0]) \n",
    "for idx, row in progress(paper_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    \n",
    "    # write the video file to disk\n",
    "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "    if r.status_code == 200:\n",
    "        with open(os.path.join('data/video/', filename), 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)       \n",
    "\n",
    "    # set a random delay, otherwise the connection gets closed\n",
    "    delay = random.randint(3, 5)\n",
    "    time.sleep(delay)\n",
    "print('=== 2 extra video files downloaded! ===')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "uf_df = metadata[metadata.source == 'UF']\n",
    "\n",
    "progress = ProgressBar(max_value=uf_df.shape[0]) \n",
    "for idx, row in progress(uf_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    \n",
    "    # write the video file to disk\n",
    "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "    if r.status_code == 200:\n",
    "        with open(os.path.join('data/video/', filename), 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)       \n",
    "\n",
    "    # set a random delay, otherwise the connection gets closed\n",
    "    delay = random.randint(3, 5)\n",
    "    time.sleep(delay)\n",
    "print('=== UF video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Scientific Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "paper_df = metadata[(metadata.source == 'Paper')] \n",
    "\n",
    "progress = ProgressBar(max_value=paper_df.shape[0]) \n",
    "for idx, row in progress(paper_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    \n",
    "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "    if r.status_code == 200:\n",
    "        with open(os.path.join('data/video/', filename), 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)       \n",
    "        \n",
    "    # set a random delay, otherwise the connection gets closed\n",
    "    if (('241_' in row.id) | ('242_' in row.id) | ('243_' in row.id)): #longer delay for last files\n",
    "        delay = random.randint(10, 20)\n",
    "    else:\n",
    "        delay = random.randint(3, 5)\n",
    "    time.sleep(delay)\n",
    "print('=== Video files extraction from papers is done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. Clarius\n",
    "* Extracting the first part of Clarius files (**6 files**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Extracting the video files...')\n",
    "clarius_df = metadata[metadata.source == 'Clarius'].iloc[:6, :]\n",
    "\n",
    "progress = ProgressBar(max_value=clarius_df.shape[0]) \n",
    "for idx, row in progress(clarius_df.iterrows()):\n",
    "    filename = row.id + '.' + row.filetype\n",
    "    \n",
    "    # write the video file to disk\n",
    "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64)'})\n",
    "    if r.status_code == 200:\n",
    "        with open(os.path.join('data/video/', filename), 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)       \n",
    "\n",
    "    # set a random delay, otherwise the connection gets closed\n",
    "    delay = random.randint(3, 5)\n",
    "    time.sleep(delay)\n",
    "print('=== Clarius video files extraction done! ===')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting the second part of Clarius files (**17 files**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print('Downloading Clarius zip file...')\n",
    "file_id = '1bqsqNzAJYwdriOP9CcGWPzCUB-7G72Ta' # sharable link id\n",
    "# zip_file_path = 'C:\\\\Users\\\\Ebadia\\\\Downloads\\\\clarius.zip' #os.path.join(get_download_path(), 'clarius.zip')\n",
    "zip_file_path = 'C:\\\\Users\\\\fpgal\\\\Downloads\\\\clarius.zip' #os.path.join(get_download_path(), 'clarius.zip')\n",
    "download_file_from_google_drive(file_id, zip_file_path)\n",
    "\n",
    "# unzip video files\n",
    "open_file = subprocess.Popen(\"utils/7z.exe\" +' x ' + zip_file_path + ' -o' + 'data/video',stdout=subprocess.PIPE)\n",
    "print('=== Clarius video files extraction done! ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Video Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move original video files to the original folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'data/video/'\n",
    "target_dir = 'data/video/original'\n",
    "    \n",
    "file_names = os.listdir(source_dir)\n",
    "\n",
    "if not os.path.exists('data/video/original'): \n",
    "    os.makedirs('data/video/original') \n",
    "\n",
    "progress = ProgressBar()\n",
    "for file_name in progress(file_names):\n",
    "    shutil.move(os.path.join(source_dir, file_name), target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Fetching Video Files Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH_ORG = 'data/video/original/'\n",
    "\n",
    "vid_files = os.listdir(VIDEO_PATH_ORG)\n",
    "\n",
    "progress = ProgressBar(max_value=metadata.shape[0]) \n",
    "with open('utils/video_files_properties.csv', 'w') as f:\n",
    "    # write the file header\n",
    "    f.write('filename,framerate,width,height,frame_count,duration_secs\\n')\n",
    "    \n",
    "    # loop over the video files and get their properties\n",
    "    for vid in progress(vid_files):\n",
    "        vid_filename = VIDEO_PATH_ORG + str(vid)\n",
    "        file_type = vid.split('.')[-1]\n",
    "        \n",
    "        # get video file properties\n",
    "        cv2video = cv2.VideoCapture(vid_filename)\n",
    "        height = cv2video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        width  = cv2video.get(cv2.CAP_PROP_FRAME_WIDTH) \n",
    "        frame_rate = round(cv2video.get(cv2.CAP_PROP_FPS), 2)\n",
    "        \n",
    "        if file_type == 'mp4':\n",
    "            frame_count = cv2video.get(cv2.CAP_PROP_FRAME_COUNT) \n",
    "            duration = round((frame_count / frame_rate), 2)\n",
    "        elif file_type == 'gif':\n",
    "            frame_count = round(Image.open(vid_filename).n_frames) #round((duration * frame_rate ), 0)\n",
    "            duration = round((frame_count / frame_rate), 2)\n",
    "\n",
    "        # write video properties to the file\n",
    "        line_to_write = str(vid) + ',' + str(frame_rate) + ',' + str(width) + ',' + str(height) + ',' + str(frame_count) + ',' + str(duration) + '\\n'\n",
    "        f.write(line_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Video Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_CROPPED_OUT = 'data/video/cropped/' #processed/cropped/'\n",
    "\n",
    "# create processed and cropped folder if they don't already exist\n",
    "if not os.path.exists('data/video/cropped'): #processed/cropped'):\n",
    "    os.makedirs('data/video/cropped') #processed/cropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Inital Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cropping metadata file\n",
    "vid_crp_metadata = pd.read_csv('utils/video_cropping_metadata.csv', sep=',', encoding='latin1')\n",
    "print(vid_crp_metadata.shape)\n",
    "vid_crp_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = ProgressBar(max_value=vid_crp_metadata.shape[0])\n",
    "\n",
    "for idx, row in progress(vid_crp_metadata.iterrows()):\n",
    "    vid_arr = []  # array to store frames of a video file\n",
    "    \n",
    "    filename = row.filename\n",
    "    file_label = filename.split('_')[-1].split('.')[0] # label of the video file\n",
    "    \n",
    "    # the following file was removed in the new release of butterfly data\n",
    "    if filename == '22_butterfly_covid.mp4':\n",
    "        continue\n",
    "    \n",
    "    cap = cv2.VideoCapture(os.path.join(VIDEO_PATH_ORG, filename))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) + 0.5)\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) + 0.5)\n",
    "    dim = (width, height) # dimension of the original file\n",
    "    \n",
    "    if pd.isna(row.x1_w_y1_h): # square cropping\n",
    "        DEL_UPPER = int(row.del_upper) # to remove top\n",
    "        WIDTH_RATE = float(row.width_rate) # to remove sides e.g. the meter\n",
    "        \n",
    "        width_border = int(width * WIDTH_RATE)\n",
    "        width_box = int(width - (2 * width_border)) \n",
    "        if width_box + DEL_UPPER > height:\n",
    "            width_box = int(height - DEL_UPPER)\n",
    "            width_border = int( (width / 2) - (width_box / 2))\n",
    "\n",
    "        while(True):\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # crop\n",
    "            frame = frame[DEL_UPPER:width_box + DEL_UPPER, width_border:width_box + width_border]\n",
    "\n",
    "            frame = np.asarray(frame).astype(np.uint8)\n",
    "            vid_arr.append(frame)\n",
    "\n",
    "    else: # crop using (x1,y1) and (x2, y2). The output will not be necessarily a square file\n",
    "        X1 = int(row.x1_w_y1_h.split(',')[0].replace('(', ''))\n",
    "        W = int(row.x1_w_y1_h.split(',')[1].strip())\n",
    "        Y1 = int(row.x1_w_y1_h.split(',')[2].strip())\n",
    "        H = int(row.x1_w_y1_h.split(',')[3].replace(')', '').strip())\n",
    "\n",
    "        while(True):\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # crop\n",
    "            frame = frame[Y1:Y1 + H, X1:X1 + W]\n",
    "\n",
    "            frame = np.asarray(frame).astype(np.uint8)\n",
    "            vid_arr.append(frame)\n",
    "\n",
    "    vid_arr = np.asarray(vid_arr)\n",
    "    # print(\"vid_arr.shape {}\".format(vid_arr.shape))\n",
    "    if (len(vid_arr.shape) != 4):\n",
    "        continue\n",
    "    prc_dim = vid_arr.shape[1:3] # dimension of the cropped file\n",
    "    prc_dim = (prc_dim[1], prc_dim[0])\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(os.path.join(VIDEO_CROPPED_OUT + filename.split('.')[0] + '_prc.avi'), fourcc, 20.0, tuple(prc_dim))\n",
    "\n",
    "    for frame in vid_arr:\n",
    "        out.write(frame.astype(\"uint8\"))\n",
    "\n",
    "    vid_crp_metadata.iloc[idx, vid_crp_metadata.columns.get_loc('crp_width')] = prc_dim[1]\n",
    "    vid_crp_metadata.iloc[idx, vid_crp_metadata.columns.get_loc('crp_height')] = prc_dim[0]\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "vid_crp_metadata.to_csv('utils/video_cropping_metadata.csv', index=None)\n",
    "\n",
    "print('Initial cropping done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Ultrasound Images from Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read video properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_prop_df = pd.read_csv('utils/video_files_properties.csv')\n",
    "\n",
    "# merge with the video meta data file \n",
    "vid_prop_df.filename = vid_prop_df.filename.astype(str)\n",
    "vid_prop_df.filename = vid_prop_df.filename.str.strip()\n",
    "\n",
    "metadata['filename2'] = metadata.id + '.' + metadata.filetype\n",
    "metadata.filename2 = metadata.filename2.astype(str)\n",
    "metadata.filename2 = metadata.filename2.str.strip()\n",
    "\n",
    "vid_prop_df = pd.merge(vid_prop_df, metadata[['filename2', 'source', 'probe', 'class']], left_on='filename', right_on='filename2', how='left').drop('filename2', axis=1)\n",
    "\n",
    "del metadata['filename2']\n",
    "print(vid_prop_df.shape)\n",
    "vid_prop_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract frames from original videos\n",
    "* v1.4.: 32,052 images are extracted\n",
    "* v1.3.: 19,161 images are extracted\n",
    "* v1.2.: 15,282 images are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH_ORG = 'data/image/original/'\n",
    "\n",
    "# create a folder for images extracted from original videos, if doesn't exist\n",
    "if not os.path.exists(IMAGE_PATH_ORG):\n",
    "    os.makedirs(IMAGE_PATH_ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_images(video_path= VIDEO_PATH_ORG, image_path=IMAGE_PATH_ORG, cropped=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Extract frames from cropped video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CROPPED_OUT = 'data/image/cropped/'\n",
    "IMAGE_MASK_OUT = 'data/mask/'\n",
    "\n",
    "# create cropped and inpainted image folders and the mask folder if they don't already exist\n",
    "if not os.path.exists(IMAGE_CROPPED_OUT):\n",
    "    os.makedirs(IMAGE_CROPPED_OUT)\n",
    "if not os.path.exists(IMAGE_MASK_OUT):\n",
    "    os.makedirs(IMAGE_MASK_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_images(video_path= VIDEO_CROPPED_OUT, image_path=IMAGE_CROPPED_OUT, cropped=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. (Optional) Extracting frames from cropped ultrasouund video files using a parameter set as filter\n",
    "* You can extract images using the follwoing parameters:\n",
    "    * maximum number of frames to be extracted from each video file\n",
    "    * extracting a targetted set of classes from ['COVID', 'Pneumonia', 'Normal', 'Other']\n",
    "    * extracting a targetted set of data sources from ['Butterfly', 'GrepMed', 'LITFL', 'PocusAtlas', 'CU', 'Radiopaedia', 'UF', 'Paper', 'Clarius']\n",
    "    * extracting a targetted set of probes from ['convex', 'linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_images(video_path= VIDEO_CROPPED_OUT, image_path=IMAGE_CROPPED_OUT, cropped=True, \n",
    "#                max_frames=10, \n",
    "#                target_class=['COVID', 'Pneumonia', 'Normal'],\n",
    "#                target_source=['Butterfly', 'GrepMed', 'LITFL', 'PocusAtlas'],\n",
    "#                target_probe=['convex', 'linear']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Preprocessing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read image preprocessing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prc_df = pd.read_csv('utils/mask_metadata.csv')\n",
    "\n",
    "image_prc_df = image_prc_df[image_prc_df.filename !='22_butterfly_covid.mp4'] # 22_butterfly_covid.mp4 was removed in March release of butterfly\n",
    "\n",
    "print(image_prc_df.shape)\n",
    "image_prc_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Removing frames with artifacts on the ROI\n",
    "Some frames of the following files need to be deleted as the moving pointer is on ROI, we will remove them from the cropped images folder:\n",
    "* 2_butterfly_covid.mp4\n",
    "* 6_butterfly_covid.mp4\n",
    "* 16_butterfly_covid.mp4\n",
    "* 20_butterfly_normal.mp4\n",
    "* 22_butterfly_covid.mp4 (it was removed in the March release of butterfly data)\n",
    "* 25_grepmed_pneumonia.mp4\n",
    "* 178_uf_other.mp4 (initial 30 frames are removed)\n",
    "* 184_uf_pneumonia.mp4 (initial 30 frames are removed)\n",
    "\n",
    "We need 2 masks for the following videos:\n",
    "* 178_uf_other.mp4 \n",
    "* 184_uf_pneumonia.mp4\n",
    "\n",
    "**Number of frames:**\n",
    "* __Initial total number of frames:__ \n",
    "    * v1.4.: 32,052\n",
    "    * v1.3.: 19,161\n",
    "    * v1.2.: 13,646\n",
    "* __Total number of frames after removal:__ \n",
    "    * v1.4.: 29,651\n",
    "    * v1.3.: 16,822\n",
    "    * v1.2.: 11,307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = ProgressBar(max_value=image_prc_df[~pd.isna(image_prc_df.delete_frames_from_to)].shape[0])\n",
    "\n",
    "for idx, row in progress(image_prc_df[~pd.isna(image_prc_df.delete_frames_from_to)].iterrows()):\n",
    "    frames_to_delete = row.delete_frames_from_to.strip().split(',')\n",
    "    frame_name_main = row.mask_main_filename.split('.')[0].split('_frame')[0]\n",
    "    \n",
    "    for frames in frames_to_delete:\n",
    "        from_frame = int(frames.split('-')[0])\n",
    "        to_frame = int(frames.split('-')[1]) + 1\n",
    "        \n",
    "        # delete frames with moving part on the roi\n",
    "        for i in range(from_frame, to_frame):\n",
    "            file_to_remove = IMAGE_CROPPED_OUT + frame_name_main + '_frame' + str(i) + '.jpg'\n",
    "            os.remove(file_to_remove)\n",
    "\n",
    "print(\"=== Files removed! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Applying masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_IMAGE_OUT = 'data/image/clean/'\n",
    "CLEAN_VIDEO_OUT = 'data/video/clean/'\n",
    "\n",
    "# create clean image and video folders if they don't already exist\n",
    "if not os.path.exists(CLEAN_IMAGE_OUT):\n",
    "    os.makedirs(CLEAN_IMAGE_OUT)\n",
    "if not os.path.exists(CLEAN_VIDEO_OUT):\n",
    "    os.makedirs(CLEAN_VIDEO_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad_array(arr, pad=5):\n",
    "    if len(arr.shape) == 3:\n",
    "        padded_arr = np.zeros((arr.shape[0]+2*pad, arr.shape[1]+2*pad, arr.shape[2]), dtype=np.uint8)\n",
    "        padded_arr[pad:pad + arr.shape[0], pad:pad + arr.shape[1], :] = arr\n",
    "    else:\n",
    "        padded_arr = np.zeros((arr.shape[0]+2*pad, arr.shape[1]+2*pad), dtype=np.uint8)\n",
    "        padded_arr[pad:pad + arr.shape[0], pad:pad + arr.shape[1]] = arr\n",
    "    return padded_arr\n",
    "        \n",
    "\n",
    "def frame_inpainting(frame_dict, mask, default_mask=0, kernel_size=(5,5), method='telea', pad=5):\n",
    "    '''\n",
    "    The function performs inpainting on frames using the created masks\n",
    "    \n",
    "    - frame_dict: dict of frames from video, indexed by frame number\n",
    "    - mask: (h, w, 1) array if single mask, else dict of such arrays\n",
    "        indexed by frame number\n",
    "    - default_mask: index for mask to be used as default, for frames\n",
    "        without specific mask (if mask is not constant across frames)\n",
    "    - kernel_size: Size of patch used to perform inpainting\n",
    "    - method: one of 'ns' (navier-stokes) or 'telea' - telea usually works better\n",
    "    '''\n",
    "    # Dilate mask make sure it covers enough of the ROI to be masked\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    if type(mask) is not dict:\n",
    "        mask = {default_mask: mask}\n",
    "    masks_processed = {key:cv2.dilate(zero_pad_array(m, pad=pad), kernel, iterations=1) for key, m in mask.items()}\n",
    "    \n",
    "    method_dict = {'ns':cv2.INPAINT_NS, 'telea':cv2.INPAINT_TELEA}\n",
    "    \n",
    "    frames_inpainted = {}\n",
    "    for key, frame in frame_dict.items():\n",
    "        if key in masks_processed:\n",
    "            #print(frame.shape, masks_processed[key].shape)\n",
    "            frames_inpainted[key] = cv2.inpaint(zero_pad_array(frame, pad=pad), masks_processed[key], 3, method_dict[method])[pad:-pad, pad:-pad, :]\n",
    "        else: # default mask\n",
    "            frames_inpainted[key] = cv2.inpaint(zero_pad_array(frame, pad=pad), masks_processed[default_mask], 3, method_dict[method])[pad:-pad, pad:-pad, :]\n",
    "\n",
    "    return frames_inpainted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = ProgressBar(max_value=image_prc_df.shape[0])\n",
    "\n",
    "for idx, row in progress(image_prc_df.iterrows()):    \n",
    "    if idx < 215:\n",
    "        continue\n",
    "\n",
    "    # get the main token of the filename\n",
    "    if row.probe == 'Convex':\n",
    "        filename_main = row.filename.split('.')[0] + '_prc_convex'\n",
    "    elif row.probe == 'Linear':\n",
    "        filename_main = row.filename.split('.')[0] + '_prc_linear'\n",
    "        \n",
    "    if row.tight_inpainting == 'yes':\n",
    "        # objects close to ROI, avoid bleeding while inpainting\n",
    "        inpainting_kernel_size = (1,1)\n",
    "    else:\n",
    "        # no objects close to ROI, more effective inpainting\n",
    "        inpainting_kernel_size = (5,5)\n",
    "\n",
    "    # check if the cropped frames need cleaning\n",
    "    if row.need_mask_after_crop == 'no':\n",
    "        frames = {}\n",
    "        \n",
    "        # 1. no clearning, copy cropped images and rename them to clean folder\n",
    "        for file in os.listdir(IMAGE_CROPPED_OUT):\n",
    "            if file.startswith(filename_main):\n",
    "                #last_part = file.split('_')[-1]\n",
    "                #last_part = last_part.replace('frame', '_clean_frame')\n",
    "                new_filename = file.replace('frame', 'clean_frame')\n",
    "                #print(file, new_filename) #, last_part)\n",
    "                shutil.copy(IMAGE_CROPPED_OUT + file, CLEAN_IMAGE_OUT + new_filename)\n",
    "\n",
    "                img = cv2.imread(os.path.join(CLEAN_IMAGE_OUT, new_filename))\n",
    "                frame_num = int(re.search(r'\\d+$', file[:-4]).group())\n",
    "                frames[frame_num] = img\n",
    "        \n",
    "        # make a video out of the clean frames\n",
    "        keys = list(frames.keys())\n",
    "        keys.sort()\n",
    "        clean_vid_frames = [frames[k] for k in keys]\n",
    "\n",
    "        h, w, layers = clean_vid_frames[0].shape\n",
    "        size = (w, h)\n",
    "\n",
    "        out = cv2.VideoWriter(os.path.join(CLEAN_VIDEO_OUT + filename_main + '_clean.avi'), cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "        for i in range(len(clean_vid_frames)):\n",
    "            out.write(clean_vid_frames[i])\n",
    "        out.release()    \n",
    "        \n",
    "    # 2. frames need cleaning\n",
    "    else: \n",
    "        # create a dictionary of frames\n",
    "        frames = {}\n",
    "        for f in os.listdir(IMAGE_CROPPED_OUT):\n",
    "            if f.startswith(filename_main):\n",
    "                img = cv2.imread(os.path.join(IMAGE_CROPPED_OUT, f))\n",
    "                frame_num = int(re.search(r'\\d+$', f[:-4]).group())\n",
    "                frames[frame_num] = img\n",
    "\n",
    "        # check if the video file requires multiple masks or a single mask is enough\n",
    "        if row.need_multiple_masks == 'no':\n",
    "            mask = cv2.imread(os.path.join(IMAGE_MASK_OUT, filename_main + '_frame0_mask.jpg'))[:,:,0]\n",
    "\n",
    "            # perform inpainting on frames using a single main mask\n",
    "            frames_inpainted = frame_inpainting(frames, mask, kernel_size=inpainting_kernel_size)\n",
    "        else:\n",
    "            masks = {}\n",
    "           \n",
    "            for f in os.listdir(IMAGE_MASK_OUT):\n",
    "                if f.startswith(filename_main):\n",
    "                    img = cv2.imread(os.path.join(IMAGE_MASK_OUT, f))\n",
    "                    frame_num = int(re.search(r'\\d+$', f[:-9]).group())\n",
    "                    masks[frame_num] = img[:,:,0]\n",
    "\n",
    "            # perform inpainting on frames using multiple masks\n",
    "            frames_inpainted = frame_inpainting(frames, masks, default_mask=0, kernel_size=inpainting_kernel_size)\n",
    "\n",
    "        # write clean frames to the disk\n",
    "        for key, value in frames_inpainted.items():\n",
    "            cv2.imwrite(CLEAN_IMAGE_OUT + filename_main + \"_clean_frame\" + str(key) + \".jpg\", value)\n",
    "\n",
    "        # write clean video to the disk\n",
    "        keys = list(frames_inpainted.keys())\n",
    "        keys.sort()\n",
    "        clean_vid_frames = [frames_inpainted[k] for k in keys]\n",
    "\n",
    "\n",
    "        #print(\"clean_vid_frames {}\".format(clean_vid_frames))\n",
    "        #print(\"clean_vid_frames[0] {}\".format(clean_vid_frames[0]))\n",
    "        #if clean_vid_frames is None:\n",
    "            #continue\n",
    "        #print(\"clean_vid_frames[0].shape {}\".format(clean_vid_frames[0].shape))\n",
    "        h, w, layers = clean_vid_frames[0].shape\n",
    "        size = (w, h)\n",
    "\n",
    "        out = cv2.VideoWriter(os.path.join(CLEAN_VIDEO_OUT + filename_main + '_clean.avi'), cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "        for i in range(len(clean_vid_frames)):\n",
    "            out.write(clean_vid_frames[i])\n",
    "        out.release()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
